{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-21 00:06:30.116682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import string\n",
    "import os\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m         f\u001B[38;5;241m.\u001B[39mwrite(r\u001B[38;5;241m.\u001B[39mcontent)\n\u001B[1;32m      8\u001B[0m processor \u001B[38;5;241m=\u001B[39m spacy\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men_core_web_sm\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 9\u001B[0m processed_text \u001B[38;5;241m=\u001B[39m processor(\u001B[43mtext\u001B[49m)\n\u001B[1;32m     11\u001B[0m processed_sentences \u001B[38;5;241m=\u001B[39m [sentence\u001B[38;5;241m.\u001B[39mlemma_\u001B[38;5;241m.\u001B[39msplit() \u001B[38;5;28;01mfor\u001B[39;00m sentence \u001B[38;5;129;01min\u001B[39;00m processed_text\u001B[38;5;241m.\u001B[39msents]\n\u001B[1;32m     12\u001B[0m cleaned_sentences \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mNameError\u001B[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "#  Preprocessing\n",
    "if not os.path.exists('./pg64317.txt'):\n",
    "    url = 'https://www.gutenberg.org/cache/epub/64317/pg64317.txt'\n",
    "    r = requests.get(url)\n",
    "    with open('pg64317.txt', 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "processor = spacy.load('en_core_web_sm')\n",
    "processed_text = processor(text)\n",
    "\n",
    "processed_sentences = [sentence.lemma_.split() for sentence in processed_text.sents]\n",
    "cleaned_sentences = []\n",
    "\n",
    "for sentence in processed_sentences:\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_sentence = [word.translate(translator) for word in sentence if word != '']\n",
    "    cleaned_sentences.append(cleaned_sentence)\n",
    "\n",
    "\n",
    "# stemmer = PorterStemmer()\n",
    "# processed_sentences = [[stemmer.stem(word) for word in sentence] for sentence in processed_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word2vec model\n",
    "model = Word2Vec(\n",
    "    sentences=cleaned_sentences,\n",
    "    vector_size=300,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "model.save('word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_1 = \"I like your shirt\"\n",
    "SENTENCE_2 = \"I love your shirt\"\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenized_sentence_1 = SENTENCE_1.split()\n",
    "tokenized_sentence_2 = SENTENCE_2.split()\n",
    "\n",
    "# # Stem the tokens\n",
    "# stemmed_sentence_1 = [stemmer.stem(word) for word in tokenized_sentence_1]\n",
    "# stemmed_sentence_2 = [stemmer.stem(word) for word in tokenized_sentence_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors\n",
    "def compute_sentence_vector(sentence : list, model):\n",
    "    vector = np.zeros(model.wv.vector_size)\n",
    "    count = 0\n",
    "    \n",
    "    for word in sentence:\n",
    "        if word in model.wv.index_to_key:\n",
    "            print(word)\n",
    "            vector += model.wv[word]\n",
    "            count += 1\n",
    "\n",
    "    if count != 0:\n",
    "        vector /= count\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load('word2vec.model')\n",
    "\n",
    "sentence1_vec = compute_sentence_vector(tokenized_sentence_1, model)\n",
    "sentence2_vec = compute_sentence_vector(tokenized_sentence_2, model)\n",
    "similarity = cosine_similarity(sentence1_vec.reshape(1, -1), sentence2_vec.reshape(1, -1))[0][0]\n",
    "print(\"Cosine Similarity: {:.4f}\".format(similarity))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = 'I'\n",
    "\n",
    "all_words = [word for word, _ in model.wv.key_to_index.items()]\n",
    "similarities = [(word, model.wv.similarity(target_word, word)) for word in all_words if word != target_word]\n",
    "\n",
    "# Sort by similarity\n",
    "sorted_similarities = sorted(similarities, key=lambda x: x[1])\n",
    "\n",
    "# Get the least similar words\n",
    "least_similar_words = sorted_similarities[:5]\n",
    "\n",
    "# Print the results\n",
    "for word, similarity in least_similar_words:\n",
    "    print(f\"{word}: {similarity}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csye7380",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
